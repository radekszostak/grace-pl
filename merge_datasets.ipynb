{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea40e3ac-9b6b-4ced-adc9-88ececd93198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0db6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/network_input\"):\n",
    "    os.makedirs(\"data/network_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34618edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(column):\n",
    "    min_val = min(column.apply(lambda d: np.min(d)))\n",
    "    max_val = max(column.apply(lambda d: np.max(d)))\n",
    "    column = column.apply(lambda d: (d - min_val) / (max_val - min_val))\n",
    "    return column, min_val, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b39c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess_numeric_data(data_dir, prefix):\n",
    "    pickle = pd.read_pickle(data_dir)\n",
    "    pickle[\"value\"] = pickle[\"value\"].apply(lambda x: np.nan_to_num(x=x, nan=np.nanmean(pickle[\"value\"])))\n",
    "    pickle[\"value\"], min_val, max_val = normalize_data(pickle[\"value\"])\n",
    "    pickle = pickle.rename(columns={\"value\": prefix + \"_value\"})\n",
    "    pickle = pickle.sort_values(by=\"date\", ignore_index=True)\n",
    "    pickle = pickle.set_index(\"date\")\n",
    "    return pickle, min_val, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4b8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess_data(data_dir, prefix):\n",
    "    pickle = pd.read_pickle(data_dir)\n",
    "    pickle[\"value\"] = pickle[\"value\"].apply(lambda x: np.nan_to_num(x=x, nan=np.nanmean(x)))\n",
    "    pickle[\"value\"], _, _ = normalize_data(pickle[\"value\"])\n",
    "    pickle = pickle.rename(columns={\"value\": prefix + \"_value\"})\n",
    "    pickle = pickle.sort_values(by=\"date\", ignore_index=True)\n",
    "    pickle = pickle.set_index(\"date\")\n",
    "    return pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe0451-3978-423b-8335-ab8afc34b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess_data_with_mask(data_dir, prefix):\n",
    "    pickle = pd.read_pickle(data_dir)\n",
    "    pickle[\"value\"] = pickle[\"value\"].apply(lambda x: np.nan_to_num(x=x, nan=np.nanmean(x)))\n",
    "    pickle[\"value\"], _, _ = normalize_data(pickle[\"value\"])\n",
    "    pickle[\"mask\"] = pickle[\"mask\"].apply(lambda x: x.astype(float))\n",
    "    pickle = pickle.rename(columns={\"value\": prefix + \"_value\", \"mask\": prefix + \"_mask\"})\n",
    "    pickle = pickle.sort_values(by=\"date\", ignore_index=True)\n",
    "    pickle = pickle.set_index(\"date\")\n",
    "    return pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd6950-4d7a-4900-b860-456d93168757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_soil_lvls(lvl1_dir, lvl2_dir, lvl3_dir, lvl4_dir):\n",
    "    \n",
    "    return read_and_preprocess_data(lvl1_dir, \"lvl1\"), read_and_preprocess_data(lvl2_dir, \"lvl2\"), read_and_preprocess_data(lvl3_dir, \"lvl3\"), read_and_preprocess_data(lvl4_dir, \"lvl4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b9ea17-541a-49c4-905e-25d619e18e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_value(nans, idx):\n",
    "    l = []\n",
    "    for i in range(nans):\n",
    "        arr = np.full(shape=(8, 13), fill_value=-999999)\n",
    "        l.append(arr)\n",
    "    l = pd.Series(l, idx)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215123f-835b-453b-9589-441eafcad24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_nans(nans, idx):\n",
    "    l = []\n",
    "    for i in range(nans):\n",
    "        arr = np.ones(shape=(8, 13), dtype=np.float32)\n",
    "        arr[0] = 0.0\n",
    "        l.append(arr)\n",
    "    l = pd.Series(l, idx)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f7b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_with_zero(a):\n",
    "    where_are_NaNs = np.isnan(a)\n",
    "    a[where_are_NaNs] = 0\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92560cb3",
   "metadata": {},
   "source": [
    "Files generated using prepare_input.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e60a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "II_112_1 = \"II_112_1.pickle\"\n",
    "II_113_1 = \"II_113_1.pickle\"\n",
    "II_114_1 = \"II_114_1.pickle\"\n",
    "II_115_1 = \"II_115_1.pickle\" # too few data\n",
    "II_116_1 = \"II_116_1.pickle\" # too few data\n",
    "II_131_1 = \"II_131_1.pickle\"\n",
    "II_132_1 = \"II_132_1.pickle\"\n",
    "II_292_1 = \"II_292_1.pickle\"\n",
    "II_297_1 = \"II_297_1.pickle\"\n",
    "II_298_1 = \"II_298_1.pickle\"\n",
    "II_472_1 = \"II_472_1.pickle\"\n",
    "II_922_1 = \"II_922_1.pickle\" # too few data\n",
    "II_924_1 = \"II_924_1.pickle\"\n",
    "II_931_1 = \"II_931_1.pickle\"\n",
    "II_932_1 = \"II_932_1.pickle\" # too few data\n",
    "II_936_1 = \"II_936_1.pickle\" # too few data\n",
    "II_940_1 = \"II_940_1.pickle\"\n",
    "II_949_1 = \"II_949_1.pickle\" # too few data\n",
    "II_951_1 = \"II_951_1.pickle\" # too few data\n",
    "II_952_1 = \"II_952_1.pickle\"\n",
    "II_957_1 = \"II_957_1.pickle\" # too few data\n",
    "II_1345_1 = \"II_1345_1.pickle\"\n",
    "II_1346_1 = \"II_1346_1.pickle\"\n",
    "II_1351_1 = \"II_1351_1.pickle\"\n",
    "II_1352_1 = \"II_1352_1.pickle\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f522de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = [II_112_1, II_113_1, II_114_1, II_131_1, II_132_1, II_292_1, II_297_1, II_298_1, II_472_1, II_924_1, II_931_1, II_940_1, II_952_1, II_1345_1, II_1346_1, II_1351_1, II_1352_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3d3f6-38ab-48a0-9f85-da098932bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "\n",
    "era_5_evap_dir = os.path.join(data_path,\"era5_evaporation.pickle\")\n",
    "era_5_soil_lvl1_dir = os.path.join(data_path, \"era5_vol_soil_lvl_1.pickle\")\n",
    "era_5_soil_lvl2_dir = os.path.join(data_path, \"era5_vol_soil_lvl_2.pickle\")\n",
    "era_5_soil_lvl3_dir = os.path.join(data_path, \"era5_vol_soil_lvl_3.pickle\")\n",
    "era_5_soil_lvl4_dir = os.path.join(data_path, \"era5_vol_soil_lvl_4.pickle\")\n",
    "precip_dir = os.path.join(data_path, \"gpm-imerg_df.pickle\")\n",
    "#precip_dir = os.path.join(data_path, \"gpm-imerg_weekly_df.pickle\")\n",
    "grace_dir = os.path.join(data_path, \"grace_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056a3d2-f020-4360-b508-e78160374a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaporation_data = read_and_preprocess_data(era_5_evap_dir, \"evap\")\n",
    "lvl1, lvl2, lvl3, lvl4 = read_soil_lvls(era_5_soil_lvl1_dir, era_5_soil_lvl2_dir, era_5_soil_lvl3_dir, era_5_soil_lvl4_dir)\n",
    "precip_data = read_and_preprocess_data(precip_dir, \"precip\")\n",
    "grace_data = read_and_preprocess_data_with_mask(grace_dir,\"grace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98956847",
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2468b948",
   "metadata": {},
   "source": [
    "Function for generating full dataset (including all features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_data(data_path, station, evaporation_data, lvl1, lvl2, lvl3, lvl4, precip_data, grace_data):\n",
    "    target_dir = os.path.join(data_path, \"measurements\", station)\n",
    "    target = pd.read_pickle(target_dir)\n",
    "    target = target.rename(columns = {\"value\": \"target_value\"})\n",
    "    min_val = target[\"target_value\"].min()\n",
    "    max_val = target[\"target_value\"].max()\n",
    "    target[\"target_value\"] = (target[\"target_value\"] - min_val) / (max_val - min_val)\n",
    "    target = target.sort_values(by=\"date\", ignore_index=True)\n",
    "    target = target.set_index(\"date\")\n",
    "    dfs = [evaporation_data, lvl1, lvl2, lvl3, lvl4, precip_data, grace_data, target]\n",
    "    data = pd.concat(dfs, axis = 1)\n",
    "    mask = data['target_value'].isna()\n",
    "    groups = (mask != mask.shift()).cumsum()\n",
    "\n",
    "    valid_groups = groups[~mask]\n",
    "    group_lengths = valid_groups.groupby(valid_groups).size()\n",
    "    longest_group_index = group_lengths.idxmax()\n",
    "    longest_group_mask = (groups == longest_group_index) & (~mask)\n",
    "    longest_group = data[longest_group_mask]\n",
    "\n",
    "    data = longest_group\n",
    "\n",
    "    data[\"merged\"] = data.apply(lambda row: np.hstack((row.loc[data.columns != \"target_value\"])), axis=1)\n",
    "    data[\"merged\"] = data[\"merged\"].apply(lambda x: np.hstack((x))) \n",
    "    columns_to_drop = [x for x in data.columns if x not in [\"target_value\", \"merged\"]]\n",
    "    data = data.drop(columns=columns_to_drop)\n",
    "    data.to_pickle(\"data/network_input/full_\" + station)\n",
    "    normalization_values = pd.DataFrame({\"min\": [min_val], \"max\": [max_val]})\n",
    "    normalization_values.to_pickle(\"data/normalization/normalization_values_\" + station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b79cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in stations:\n",
    "    generate_full_data(data_path, station, evaporation_data, lvl1, lvl2, lvl3, lvl4, precip_data, grace_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f5d01",
   "metadata": {},
   "source": [
    "Function for generating dataset with only GRACE feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eef385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grace_data(data_path, station, grace_data):\n",
    "    target_dir = os.path.join(data_path, \"measurements\", station)\n",
    "    target, _, _ = read_and_preprocess_numeric_data(target_dir, \"target\")\n",
    "    dfs = [grace_data, target]\n",
    "    data = pd.concat(dfs, axis = 1)\n",
    "    mask = data['target_value'].isna()\n",
    "    groups = (mask != mask.shift()).cumsum()\n",
    "\n",
    "    valid_groups = groups[~mask]\n",
    "    group_lengths = valid_groups.groupby(valid_groups).size()\n",
    "    longest_group_index = group_lengths.idxmax()\n",
    "    longest_group_mask = (groups == longest_group_index) & (~mask)\n",
    "    longest_group = data[longest_group_mask]\n",
    "\n",
    "    data = longest_group\n",
    "\n",
    "    data[\"merged\"] = data.apply(lambda row: np.hstack((row.loc[data.columns != \"target_value\"])), axis=1)\n",
    "    data[\"merged\"] = data[\"merged\"].apply(lambda x: np.hstack((x))) \n",
    "    columns_to_drop = [x for x in data.columns if x not in [\"target_value\", \"merged\"]]\n",
    "    data = data.drop(columns=columns_to_drop)\n",
    "    data.to_pickle(\"data/network_input/grace_\" + station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in stations:\n",
    "    generate_grace_data(data_path, station, grace_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
